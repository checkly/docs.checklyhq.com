---
title: OpenTelemetry metrics
subTitle: An introduction to OpenTelemetry's most data-efficient signal
displayTitle: OpenTelemetry Metrics
description: OpenTelemetry Metrics play a critical role in monitoring applications by offering a way to capture and analyze key metrics in a standardized, scalable manner. Whether you're managing a complex microservices architecture or a simpler system, OpenTelemetry helps track essential statistics that reveal the health and performance of your services.
date: 2024-10-18
author: Nocnica Mellifera
githubUser: serverless-mom
displayDescription: 
  Learn more about OpenTelemetry & Monitoring with Checkly. Explore metrics, one of the three pillars of observability.
menu:
  learn:
    parent: "OpenTelemetry"
weight: 3
---

**OpenTelemetry Metrics** play a critical role in monitoring applications by offering a way to capture and analyze key metrics in a standardized, scalable manner. Whether you're managing a complex microservices architecture or a simpler system, OpenTelemetry helps track essential statistics that reveal the health and performance of your services.

---

## What are Metrics?

Metrics represent **quantitative measurements** of your system’s health and behavior. They provide insights into performance trends, such as:

- **CPU usage** over time
- **Request rates** per endpoint
- **Error counts** or failure rates
- **Latency** in handling requests

Metrics are lightweight and highly efficient to collect, aggregate, and query. They help identify patterns and anomalies without burdening storage, making them suitable for continuous monitoring at scale.

### Types of Metrics in OpenTelemetry:

- **Counter**: Measures occurrences or events, such as the number of requests handled.
- **Gauge**: Captures values that fluctuate, like memory usage.
- **Histogram**: Measures the distribution of values, such as response time percentiles.

Explore further in the [OpenTelemetry Metrics Documentation](https://opentelemetry.io/docs/concepts/signals/metrics/).


## Why Metrics Matter

In a **microservices** environment, metrics are indispensable for:

- **Performance monitoring**: Identifying bottlenecks or degraded performance.
- **Capacity planning**: Forecasting when additional resources are required.
- **Incident detection**: Alerting teams about abnormal system behavior.

Metrics are often **the first step** in identifying that something has gone wrong. If a metric shows unusual values (e.g., a spike in response time), you can investigate further by drilling into traces or logs to find the root cause.

## Metrics vs. Traces

Metrics have a number of advantages over tracing. Metrics are much more data efficient, generally at the collector level it’s possible to compress hundreds of individual metrics reported to a single packet of data sent on to the metrics backend. Further, metrics show broad trends whereas a trace, no matter how interesting, will always cover only a single request.

Should you use metrics instead of traces to monitor your service? Absolutely not. Metrics will always present average performance, and the specific information needed to really understand root causes will be elusive. Further, even with high resolution timeseries metrics it’s very hard to go from worrying metrics to find matching log data of a problem. Finally, modern traces can effectively show information about asynchronous requests as they contribute to overall request time, something that’s very hard to tease out of bare metrics.

## Setting up OpenTelemetry Metrics

### Auto-Instrumentation vs. Manual Instrumentation

1. **Auto-Instrumentation**: Many popular frameworks and libraries come with automatic OpenTelemetry instrumentation, requiring minimal setup.
2. **Manual Instrumentation**: Developers can manually add metrics within the application code by using SDKs to track specific business metrics (e.g., purchases per hour).

Learn more about instrumentation options in the [OpenTelemetry SDK Guide](https://opentelemetry.io/docs/instrumentation/).

## Example Metric Pipeline

With OpenTelemetry, you can collect, process, and export metrics using **Collectors**. Here’s a high-level example of a typical metric pipeline:

1. **Data Collection**: Metrics are generated by instrumented services.
2. **Processing**: The OpenTelemetry Collector aggregates and processes the data (e.g., batching or filtering metrics).
3. **Exporting**: Metrics are sent to observability platforms like **Prometheus** or **Grafana**.

Learn how to configure a collector in the [OpenTelemetry Collector Guide](learn/opentelemetry/otel-collector/).



## Best Practices for Metrics in OpenTelemetry

- **Optimize cardinality**: Avoid creating too many distinct labels, as this can overwhelm storage and query systems.
- **Set appropriate aggregation intervals**: Batch data intelligently to balance between real-time insights and system load.
- **Use meaningful names**: Clearly describe the purpose of each metric to make dashboards and alerts easier to understand.
- **Standardize naming early**: While OpenTelemetry defines standard language for a number of concepts, actual metric naming is not standardized. As such it's possible to report `total-web-shop-checkout-time` and `webShopCheckoutTime_total` as two totally separate metrics even though they should be aggregated. No standard is perfect, of course, and to normalize data before it's stored, use the [filtering tools in the OpenTelemetry collector](learn/opentelemetry/otel-filtering/).


OpenTelemetry metrics provide a robust foundation for observability, helping teams proactively monitor performance and detect issues before they escalate. With the right setup and tooling, you can gain comprehensive insights into your applications, enabling faster resolution times and improved reliability.

OpenTelemetry offers two main strategies for adding observability to your application: **direct instrumentation** and **auto-instrumentation**. Each method serves specific use cases, allowing developers to capture telemetry data efficiently while balancing flexibility and simplicity.

---

### Direct Instrumentation

Direct instrumentation involves **manually adding code** to your application to generate telemetry data. This requires developers to use the OpenTelemetry SDK specific to their programming language, explicitly invoking functions to create traces, metrics, or logs.

- **Example Use Case**: You might define spans that wrap critical sections of code to monitor database queries or key service calls.
- **Advantages**:
    1. **Precise Control**: Developers control which parts of the application generate telemetry data.
    2. **Contextual Richness**: Since code is written by hand, you can include detailed metadata relevant to your business logic.
    3. **Customization**: Allows for intricate tracking of custom events, metrics, and logs that fit your specific needs.

This approach ensures deeper insights but requires more engineering effort upfront and ongoing maintenance to ensure proper coverage and updates as the code evolves. There are of course 1,001 ways that a clever developer could ensure that the intervention needed even for direct instrumentation was miniman. For example, by wrapping the function that first receives a request, and the function class to ensure that future function calls are reported to a tracer. Direct instrumentation doesn't mean modifying n+1 lines of code, but it does require a fair amount of effort to get launched the first time, no matter how clever your developers are. 

### Auto-Instrumentation

Auto-instrumentation eliminates the need for manual code changes by **injecting telemetry at runtime**. This is often achieved through agents or middleware that modify existing code dynamically. For example, Java or Python applications can use agents to hook into libraries and frameworks they rely on.

- **Example Use Case**: A web server library automatically generates spans for HTTP requests without developer involvement.
- **Advantages**:
    1. **Fast Setup**: No changes to the codebase are required, making it easy to get started.
    2. **Coverage for Standard Libraries**: Many popular libraries and frameworks come with built-in support for auto-instrumentation.
    3. **Consistency**: Ensures consistent telemetry for common operations, reducing the risk of missing critical observability data.
    
    Auto-instrumentation may not offer the same level of granularity or customization as manual instrumentation. Automatic instrumentation can produce unnecessarily ‘noisy’ instrumentation, as just one example the NodeJS auto-instrumentation can produce traces with [hundreds of file system spans](https://github.com/open-telemetry/opentelemetry-js-contrib/issues/1344) that can even overrun available memory. 
    
    Finally, auto-instrumentation won’t generally produce useful results if something really unusual is happening inside your application. At base, automatic instrumentation assumes that your service is some kind of online application handling requests and sending responses. If that’s not the case, for example if your application is a machine learning system processing through a database and taking over 48 hours to produce results, you may get less useful information. In these cases, you may want to look into manually reporting [OpenTelemetry metrics](/learn/opentelemetry/otel-metrics).
    

### When to Use Each

- **Direct instrumentation** is ideal for teams seeking maximum flexibility, particularly for custom workflows or applications with unique behavior.
- **Auto-instrumentation** is better suited for scenarios where rapid observability is needed, or when standard libraries provide sufficient coverage.

In many cases, these methods can complement each other. Start with auto-instrumentation to gain quick insights, then enhance with direct instrumentation to capture application-specific details.

For more on how to add instrumentation to specifically integrate with Checkly Traces, [see our Traces documentation](https://www.checklyhq.com/docs/traces-open-telemetry/).